"""
Exa API Client for product discovery and web crawling
Ported from old project with enhancements for interior design recommendations
"""

import os
import re
from typing import Any, Dict, List, Optional
from urllib.parse import urlparse

from dotenv import load_dotenv
from exa_py import Exa

load_dotenv()


class ExaClient:
    """Client for Exa API to find and analyze furniture products online"""

    def __init__(self, api_key: str = None):
        """Initialize Exa client with API key"""
        if Exa is None:
            raise ImportError(
                "exa-py package not installed. Install with: pip install exa-py"
            )

        self.api_key = api_key or os.getenv("EXA_API_KEY")
        if not self.api_key:
            raise ValueError("EXA_API_KEY not found in environment variables")

        self.exa = Exa(self.api_key)

    def search_products(self, query: str, num_results: int = 10) -> Dict[str, Any]:
        """
        Search for furniture products based on a text query

        Args:
            query: Search query (e.g., "modern gray sectional sofa")
            num_results: Number of results to return

        Returns:
            Search results from Exa
        """
        # Focus on trusted furniture retailers
        trusted_domains = [
            "wayfair.com",
            "westelm.com",
            "cb2.com",
            "crateandbarrel.com",
            "potterybarn.com",
            "article.com",
            "ikea.com",
            "amazon.com",
            "target.com",
            "homedepot.com",
            "walmart.com",
            "overstock.com",
            "allmodern.com",
            "ashleyfurniture.com",
            "roomstogo.com",
            "livingspaces.com",
            "pier1.com",
            "homegoods.com",
        ]

        try:
            print(f"\n沐 DEBUG: Exa search query: '{query}'")
            print(
                f"沐 DEBUG: Requesting {num_results} results (actual API call: {num_results * 2})"
            )

            # Use the official Exa SDK
            # Note: Can't use both include_domains and exclude_domains with content
            # Try search without content first to get better URLs
            basic_result = self.exa.search(
                query=query,
                num_results=num_results,
                use_autoprompt=True,
                type="auto",
                include_domains=trusted_domains,
            )

            print(
                f"沐 DEBUG: Basic search returned {len(basic_result.results) if basic_result.results else 0} results"
            )

            # Get content for fewer results to avoid rate limits
            result = self.exa.get_contents(
                urls=[
                    r.url for r in basic_result.results[:5]
                ],  # Only get content for first 5 URLs
                text={
                    "max_characters": 4000,  # Reduced to avoid timeouts
                    "include_html_tags": True,
                },
            )

            print(
                f"沐 DEBUG: Got content for {len(result.results) if result.results else 0} results"
            )
            if basic_result.results:
                print("沐 DEBUG: First basic result:")
                first = basic_result.results[0]
                print(f"  - URL: {first.url}")
                print(f"  - Title: {first.title}")
                print(f"  - ID: {first.id}")

                # Check if we got content for this result
                debug_content_by_url = (
                    {item.url: item for item in result.results}
                    if result.results
                    else {}
                )
                content_item = debug_content_by_url.get(first.url)
                if content_item:
                    print("  - Has content: yes")
                    print(
                        f"  - Text length: {len(content_item.text) if hasattr(content_item, 'text') and content_item.text else 0} chars"
                    )
                    if hasattr(content_item, "text") and content_item.text:
                        print(f"  - Text preview: {content_item.text[:200]}...")
                else:
                    print("  - Has content: no")

            # Convert to our expected format
            converted_results = {"results": []}

            # Create content mapping by URL for all results
            content_by_url = (
                {item.url: item for item in result.results} if result.results else {}
            )

            for basic_item in basic_result.results:
                content_item = content_by_url.get(basic_item.url)
                converted_item = {
                    "url": basic_item.url,
                    "title": basic_item.title,
                    "id": basic_item.id,
                    "published_date": basic_item.published_date,
                    "author": basic_item.author,
                    "extract": getattr(content_item, "extract", "")
                    if content_item
                    else "",
                    "text": getattr(content_item, "text", "") if content_item else "",
                    "highlights": getattr(content_item, "highlights", [])
                    if content_item
                    else [],
                }
                converted_results["results"].append(converted_item)

            print(f"沐 DEBUG: Converted to {len(converted_results['results'])} results")
            return converted_results

        except Exception as e:
            print(f"Error searching products: {e}")
            return {"error": str(e), "results": []}

    def get_product_details(
        self, search_results: List[Dict[str, Any]], max_results: int = 10
    ) -> List[Dict[str, Any]]:
        """
        Get detailed product information from search results

        Args:
            search_results: List of search results from Exa
            max_results: Maximum number of results to process

        Returns:
            List of product details with pricing, availability, etc.
        """
        if not search_results:
            return []

        products = []
        for result in search_results[:max_results]:
            product = self._extract_product_info(result)
            if product:
                products.append(product)

        return products

    def search_and_analyze_products(
        self, query: str, space_type: str, num_results: int = 15
    ) -> List[Dict[str, Any]]:
        """
        Complete workflow: search for products and analyze them

        Args:
            query: Product search query
            space_type: Room type (bedroom, living room, etc.)
            num_results: Number of initial search results

        Returns:
            List of analyzed products with scores and details
        """
        # Search for products
        search_results = self.search_products(query, num_results)

        if "error" in search_results:
            return []

        # Get product details from search results
        products = self.get_product_details(search_results.get("results", []))

        # Enhance with space type context
        for product in products:
            product["space_type"] = space_type
            product["search_query"] = query

        return products

    def _extract_product_info(
        self, content_result: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """Extract structured product information from content"""
        try:
            url = content_result.get("url", "")
            title = content_result.get("title", "")
            text = content_result.get("text", "")

            if not text:
                return None

            # Extract store name
            store_name = self._extract_store_name(url, title)

            # Extract price
            price_info = self._extract_price(text)

            # Extract availability
            availability = self._extract_availability(text)

            # Extract shipping info
            shipping = self._extract_shipping_info(text)

            # Extract product details
            details = self._extract_product_details(text)

            # Extract product images
            images = self._extract_product_images(text, url, store_name)

            # Format to match frontend expectations (same as SERP client)
            product = {
                # Core fields
                "title": title,
                "url": url,
                "store": store_name,  # Frontend expects "store", not "store_name"
                "price": self._parse_price_to_float(price_info["price"]),
                "price_str": price_info["price"],
                "availability": availability,
                "images": images,
                "description": content_result.get("extract", "")[:300],
                "rating": details["rating"],
                "reviews": details["reviews_count"],
                "delivery": shipping,
                # Additional fields
                "materials": details["materials"],
                "colors": details["colors"],
                "dimensions": details["dimensions"],
                # Compatibility fields
                "space_type": "",  # Will be set later
                "search_query": "",  # Will be set later
                "is_product_page": self._is_likely_product_page(url, text),
            }

            return product

        except Exception as e:
            print(f"Error extracting product info: {e}")
            return None

    def _extract_store_name(self, url: str, title: str) -> str:
        """Extract store name from URL and title"""
        known_stores = {
            "amazon.com": "Amazon",
            "wayfair.com": "Wayfair",
            "ikea.com": "IKEA",
            "westelm.com": "West Elm",
            "cb2.com": "CB2",
            "crateandbarrel.com": "Crate & Barrel",
            "potterybarn.com": "Pottery Barn",
            "article.com": "Article",
            "allmodern.com": "AllModern",
            "homedepot.com": "Home Depot",
            "target.com": "Target",
            "walmart.com": "Walmart",
            "overstock.com": "Overstock",
            "ashleyfurniture.com": "Ashley Furniture",
        }

        url_lower = url.lower()
        for domain, store_name in known_stores.items():
            if domain in url_lower:
                return store_name

        # Extract from URL hostname
        try:
            parsed = urlparse(url)
            hostname = parsed.hostname or ""
            if hostname.startswith("www."):
                hostname = hostname[4:]
            parts = hostname.split(".")
            if len(parts) >= 2:
                return parts[0].title()
        except Exception:
            pass

        return "Unknown Store"

    def _extract_price(self, text: str) -> Dict[str, str]:
        """Extract pricing information"""
        price_patterns = [
            r"\$[\d,]+\.?\d*",
            r"USD\s*[\d,]+\.?\d*",
            r"Price:\s*\$[\d,]+\.?\d*",
            r"Sale Price:\s*\$[\d,]+\.?\d*",
        ]

        prices = []
        for pattern in price_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            prices.extend(matches)

        # Look for sale/discount indicators
        original_price = None
        current_price = prices[0] if prices else "Price not available"
        discount = None

        if "was" in text.lower() and "now" in text.lower():
            was_match = re.search(r"was\s*(\$[\d,]+\.?\d*)", text, re.IGNORECASE)
            now_match = re.search(r"now\s*(\$[\d,]+\.?\d*)", text, re.IGNORECASE)
            if was_match and now_match:
                original_price = was_match.group(1)
                current_price = now_match.group(1)

        # Look for discount percentage
        discount_match = re.search(r"(\d+)%\s*off", text, re.IGNORECASE)
        if discount_match:
            discount = f"{discount_match.group(1)}% off"

        return {
            "price": current_price,
            "original_price": original_price,
            "discount": discount,
        }

    def _extract_availability(self, text: str) -> str:
        """Extract availability information"""
        text_lower = text.lower()

        if "in stock" in text_lower:
            return "In Stock"
        elif "out of stock" in text_lower:
            return "Out of Stock"
        elif "limited availability" in text_lower:
            return "Limited Availability"
        elif re.search(r"ships? in \d+ days?", text_lower):
            match = re.search(r"ships? in (\d+) days?", text_lower)
            return f"Ships in {match.group(1)} days" if match else "Check availability"
        elif "ready to ship" in text_lower:
            return "Ready to Ship"
        else:
            return "Check availability"

    def _extract_shipping_info(self, text: str) -> str:
        """Extract shipping information"""
        text_lower = text.lower()
        shipping_info = []

        if "free shipping" in text_lower:
            shipping_info.append("Free shipping")
        if "free delivery" in text_lower:
            shipping_info.append("Free delivery")

        # Look for shipping time
        shipping_match = re.search(r"delivery in (\d+[-\s]?\d*)\s*days?", text_lower)
        if shipping_match:
            days = shipping_match.group(1)
            shipping_info.append(f"Delivery in {days} days")

        return " | ".join(shipping_info) if shipping_info else "Standard shipping"

    def _extract_product_details(self, text: str) -> Dict[str, Any]:
        """Extract detailed product information"""
        text_lower = text.lower()

        # Materials
        materials = [
            "wood",
            "metal",
            "fabric",
            "leather",
            "glass",
            "plastic",
            "velvet",
            "cotton",
            "steel",
            "iron",
            "oak",
            "pine",
        ]
        found_materials = [mat for mat in materials if mat in text_lower]

        # Colors
        colors = [
            "red",
            "blue",
            "green",
            "yellow",
            "black",
            "white",
            "gray",
            "brown",
            "beige",
            "navy",
            "teal",
            "charcoal",
            "cream",
        ]
        found_colors = [col for col in colors if col in text_lower]

        # Dimensions
        dimensions = []
        dimension_patterns = [
            r'\d+["\']?\s*[xXﾃ余\s*\d+["\']?\s*[xXﾃ余\s*\d+["\']?',
            r'\d+\.\d+["\']?\s*[xXﾃ余\s*\d+\.\d+["\']?\s*[xXﾃ余\s*\d+\.\d+["\']?',
            r'\d+["\']?\s*[wW]\s*[xXﾃ余\s*\d+["\']?\s*[dD]\s*[xXﾃ余\s*\d+["\']?\s*[hH]',
        ]
        for pattern in dimension_patterns:
            matches = re.findall(pattern, text)
            dimensions.extend(matches)

        # Features (look for bullet points or key features)
        features = []
        if "features:" in text_lower:
            features_section = text.lower().split("features:")[1].split("\n")[:5]
            features = [f.strip("- 窶｢").strip() for f in features_section if f.strip()]

        # Rating
        rating = None
        rating_match = re.search(
            r"(\d+\.?\d*)\s*out of 5|(\d+\.?\d*)/5\s*stars?", text_lower
        )
        if rating_match:
            rating = rating_match.group(1) or rating_match.group(2)

        # Reviews count
        reviews_count = None
        reviews_match = re.search(r"(\d+[,\d]*)\s*reviews?", text_lower)
        if reviews_match:
            reviews_count = reviews_match.group(1)

        return {
            "materials": found_materials[:3],  # Top 3 materials
            "colors": found_colors[:3],  # Top 3 colors
            "dimensions": dimensions[:2],  # Top 2 dimension sets
            "features": features[:5],  # Top 5 features
            "rating": rating,
            "reviews_count": reviews_count,
        }

    def _extract_product_images(
        self, text: str, url: str, store_name: str
    ) -> List[str]:
        """Extract product image URLs from HTML content"""
        images = []

        print(f"沐 DEBUG: Extracting images from {store_name} page")
        print(f"  - Page URL: {url}")
        print(f"  - Content length: {len(text)} chars")

        # Common image URL patterns in HTML - MORE COMPREHENSIVE
        img_patterns = [
            # Standard img tags
            r'<img[^>]*src=["\'](https?://[^"\']*\.(?:jpg|jpeg|png|webp|gif)(?:\?[^"\']*)?)["\'][^>]*>',
            # Lazy loading attributes
            r'data-src=["\'](https?://[^"\']*\.(?:jpg|jpeg|png|webp|gif)(?:\?[^"\']*)?)["\']',
            r'data-lazy-src=["\'](https?://[^"\']*\.(?:jpg|jpeg|png|webp|gif)(?:\?[^"\']*)?)["\']',
            r'data-original=["\'](https?://[^"\']*\.(?:jpg|jpeg|png|webp|gif)(?:\?[^"\']*)?)["\']',
            # Srcset (responsive images)
            r'srcset=["\'](https?://[^"\']*\.(?:jpg|jpeg|png|webp|gif)(?:\?[^"\']*)?)["\']',
            # JSON-LD structured data (often contains high-quality images)
            r'"image":\s*"(https?://[^"]*\.(?:jpg|jpeg|png|webp|gif)(?:\?[^"]*)?)"',
            r'"url":\s*"(https?://[^"]*\.(?:jpg|jpeg|png|webp|gif)(?:\?[^"]*)?)"',
            # CSS background images
            r'background-image:\s*url\(["\']?(https?://[^)\'\"]*\.(?:jpg|jpeg|png|webp|gif)(?:\?[^)\'\"]*)?)["\']?\)',
            # Amazon specific patterns (try to get CDN URLs)
            r'(https://[^"\']*\.ssl-images-amazon\.com/images/[^"\']*\.(?:jpg|jpeg|png|webp))',
            r'(https://m\.media-amazon\.com/images/[^"\']*\.(?:jpg|jpeg|png|webp))',
        ]

        # Extract all potential image URLs
        for pattern in img_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            print(f"  - Pattern found {len(matches)} matches: {pattern[:50]}...")
            images.extend(matches)

        # Filter and clean images
        filtered_images = []
        for img_url in images:
            # Skip thumbnails, icons, and non-product images
            if any(
                skip_word in img_url.lower()
                for skip_word in [
                    "logo",
                    "icon",
                    "banner",
                    "header",
                    "footer",
                    "nav",
                    "thumb",
                    "avatar",
                    "profile",
                    "badge",
                    "arrow",
                    "button",
                    "spacer",
                    "1x1",
                    "pixel",
                    "tracking",
                ]
            ):
                continue

            # Look for product-like images
            if any(
                product_word in img_url.lower()
                for product_word in [
                    "product",
                    "item",
                    "furniture",
                    "chair",
                    "sofa",
                    "table",
                    "bed",
                    "desk",
                    "cabinet",
                    "shelf",
                    "lamp",
                ]
            ):
                filtered_images.append(img_url)
            # Include images from product image directories
            elif any(
                dir_word in img_url.lower()
                for dir_word in [
                    "/products/",
                    "/items/",
                    "/catalog/",
                    "/images/p/",
                    "/img/p/",
                ]
            ):
                filtered_images.append(img_url)
            # Include larger images (likely product photos)
            elif any(
                size in img_url.lower()
                for size in [
                    "400",
                    "500",
                    "600",
                    "800",
                    "1000",
                    "large",
                    "medium",
                    "main",
                ]
            ):
                filtered_images.append(img_url)

        # Store-specific image patterns
        if "wayfair" in store_name.lower():
            # Wayfair typically has product images in specific formats
            wayfair_patterns = [
                r'https://secure\.img\d*-.*?\.wfcdn\.com/[^"\s]*\.jpg',
                r'https://assets\.wfcdn\.com/[^"\s]*\.jpg',
            ]
            for pattern in wayfair_patterns:
                matches = re.findall(pattern, text)
                filtered_images.extend(matches)

        elif "ikea" in store_name.lower():
            # IKEA image patterns
            ikea_patterns = [
                r'https://www\.ikea\.com/[^"\s]*\.jpg',
                r'https://d2xjmi1k71iy2m\.cloudfront\.net/[^"\s]*\.jpg',
            ]
            for pattern in ikea_patterns:
                matches = re.findall(pattern, text)
                filtered_images.extend(matches)

        elif "amazon" in store_name.lower():
            # Amazon image patterns
            amazon_patterns = [
                r'https://m\.media-amazon\.com/images/[^"\s]*\.jpg',
                r'https://images-na\.ssl-images-amazon\.com/[^"\s]*\.jpg',
            ]
            for pattern in amazon_patterns:
                matches = re.findall(pattern, text)
                filtered_images.extend(matches)

        # Remove duplicates and return top 3 images
        unique_images = list(dict.fromkeys(filtered_images))
        final_images = unique_images[:3]

        print("沐 DEBUG: Image extraction summary:")
        print(f"  - Total URLs found: {len(images)}")
        print(f"  - After filtering: {len(filtered_images)}")
        print(f"  - Unique images: {len(unique_images)}")
        print(f"  - Final images: {len(final_images)}")

        for i, img_url in enumerate(final_images):
            print(f"  - Image {i + 1}: {img_url}")

        return final_images

    def _parse_price_to_float(self, price_str: str) -> Optional[float]:
        """Parse price string to float for frontend compatibility"""
        if not price_str or price_str == "Price not available":
            return None

        try:
            # Remove currency symbols and extract numeric value
            import re

            numeric_match = re.search(r"[\d,]+\.?\d*", price_str.replace(",", ""))
            if numeric_match:
                return float(numeric_match.group())
        except Exception:
            pass

        return None

    def _is_likely_product_page(self, url: str, text: str) -> bool:
        """Determine if this is likely a product page vs search results"""
        # Check for product page URL patterns
        product_patterns = ["/p/", "/product/", "/item/", "/pd/", "/dp/", "sku="]
        url_lower = url.lower()

        if any(pattern in url_lower for pattern in product_patterns):
            return True

        # Check for single product indicators in text
        text_lower = text.lower()
        if "add to cart" in text_lower or "buy now" in text_lower:
            return True

        # Check for multiple prices (indicates search results)
        price_count = len(re.findall(r"\$\d+", text))
        if price_count > 5:
            return False

        return True
